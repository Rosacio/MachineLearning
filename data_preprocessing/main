# importing the important libraries

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Preparing the data into two separate datasets, one depend and one independ
dataset = pd.read_csv('C:\\Users\\joao_\\Downloads\\Data.csv')
x = dataset.iloc[:, :-1].values
y = dataset.iloc[:, -1].values
print(x)
print(y)


# Filling the missing data in the dataset with the avg value of each column

from sklearn.impute import SimpleImputer
imputer  = SimpleImputer(missing_values=np.nan, strategy='mean')
imputer.fit(x[:, 1:3])
x[:, 1:3] = imputer.transform(x[:, 1:3])
print(x)


# Since this data contains categorical values we will ncoding the independent variable as
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
ct = ColumnTransformer(transformers=[('enconder',OneHotEncoder(),[0])] , remainder='passthrough')
x = np.array(ct.fit_transform(x))
print(x)

#Now instead of each country name we have a unique id for each country
#Now we are going to convert our depend variable into 0 and 1 instead of strings
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y = le.fit_transform(y)
print(y)

#Feature Scaling ALWAYS after spliting the dataset and training test
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test  = train_test_split(x, y, test_size = 0.2, random_state = 1)
print(X_train)

#Feature Scalling is very usefull, the most common features are standardisation and normalisation
#Since standardisation is more appropriate for most cases we will do standardisation
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train[:, 3:] = sc.fit_transform(X_train[:, 3:])
X_test[:, 3:] = sc.transform(X_test[:, 3:])

